# Architecture Document

## How the System Works

The voice bot is a real-time audio processing pipeline built on FastAPI with a WebSocket core. When a call is triggered via `call_runner.py`, Twilio places an outbound call to the target number and — once the call connects — fires a webhook to our server's `/incoming-call/{scenario_id}` endpoint. The server responds with TwiML instructing Twilio to open a bidirectional **Media Stream**: a persistent WebSocket connection that carries raw mulaw-encoded 8kHz audio in both directions, wrapped in JSON envelopes.

On that WebSocket, audio arriving from the agent flows into a **Deepgram Streaming STT** connection (also a WebSocket, running concurrently), which returns real-time transcripts as the agent speaks. A `TurnDetector` accumulates transcript fragments and detects when the agent has finished speaking by monitoring for 1.5 seconds of silence — at which point it surfaces the full agent utterance. That text is sent to **GPT-4o** alongside the full conversation history and a scenario-specific system prompt that defines the patient's persona, goals, and behavioral rules. GPT-4o returns a short, natural patient response (1–3 sentences, 150 token cap). That text is converted to speech via **OpenAI TTS** (producing MP3), then converted to mulaw 8kHz via ffmpeg, chunked into 80ms segments, and streamed back through the Twilio WebSocket so the caller hears the patient speaking. Every turn — both agent and patient — is logged to a JSON transcript file. After all calls are complete, `bug_analyzer.py` feeds each transcript to GPT-4o with a structured QA prompt to categorize and rate any issues found.

## Key Design Decisions

**Twilio Media Streams over TwiML `<Gather>`/`<Say>`**: The TwiML approach works for simple IVR bots but is fundamentally request/response — you can't stream audio continuously or detect natural pauses in speech. Media Streams give us a live bidirectional audio channel that enables real turn-taking. The tradeoff is higher complexity (WebSocket management, audio format conversion), but it's the only way to simulate a realistic human conversation.

**Deepgram over Whisper for STT**: Whisper is a batch transcription model — you'd have to buffer an entire utterance, then wait for transcription, adding 3–5 seconds of latency. Deepgram's streaming API returns partial and final transcripts in near real-time (~300ms), which is essential for natural conversational pacing. The nova-2 model also handles phone-quality mulaw 8kHz audio natively with no preprocessing required.

**Scenario-based system prompts over free-form**: Rather than giving GPT-4o generic instructions, each scenario gives it a specific persona (name, age, DOB, insurance, medications), a concrete goal, and behavioral constraints. This produces more realistic and reproducible conversations — and more importantly, it lets us deliberately probe specific failure modes. Scenario 5 specifically uses a specialty drug name (Jardiance) to probe hallucination; Scenario 8 deliberately makes the patient confused to test the agent's patience and clarification logic; Scenario 7 pushes for cost estimates to see if the agent makes up numbers. Free-form testing would rarely surface these edge cases consistently.

**Post-processing bug analysis**: Rather than relying solely on manual transcript review, `bug_analyzer.py` sends each transcript to GPT-4o with a structured QA taxonomy (hallucination, misunderstanding, wrong information, poor UX, missing confirmation, etc.). This provides consistent, categorized analysis across all 10 calls and surfaces patterns that might be missed when reading transcripts one at a time.
